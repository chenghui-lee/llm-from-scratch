{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb47e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install wandb torch torchvision torchaudio transformers[torch] 'accelerate>=0.26.0' tokenizers datasets[s3]\n",
    "\n",
    "# need to restart kernel before proceeding further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c789bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install tf-keras\n",
    "# for datacrunch only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login <token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e29d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast, TrainingArguments, Trainer, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_PATH = \"./fineweb-gpt2-final/\"\n",
    "TOKENIZER_FILE = \"fineweb-10bt-tokenizer-bpe.json\"\n",
    "OUTPUT_DIR = \"./gpt2-chat-alpaca-dolly-2-neft-0p3\"\n",
    "ALPACA_DATASET_NAME = \"tatsu-lab/alpaca\"\n",
    "DOLLY_DATASET_NAME = \"databricks/databricks-dolly-15k\"\n",
    "SEED = 3047\n",
    "MAX_LENGTH = 1024\n",
    "NEFTUNE_NOISE_ALPHA = 5.0\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=TOKENIZER_FILE,\n",
    "    bos_token=\"<|im_start|>\",\n",
    "    eos_token=\"<|im_end|>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")\n",
    "\n",
    "# add special token\n",
    "special_tokens_to_add = []\n",
    "if tokenizer.bos_token_id is None:\n",
    "    special_tokens_to_add.append(\"<|im_start|>\")\n",
    "if tokenizer.eos_token_id is None:\n",
    "    special_tokens_to_add.append(\"<|im_end|>\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "     # fallback to eos token\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "        special_tokens_to_add.append(\"<pad>\")\n",
    "\n",
    "\n",
    "if special_tokens_to_add:\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": list(set(special_tokens_to_add))})\n",
    "\n",
    "\n",
    "# i use chatML here, other format such as alpaca is available\n",
    "chat_template = \"\"\"{% for message in messages -%}\n",
    "{{ bos_token }}{{ message['role'] }}\n",
    "{{ message['content'] }}{{ eos_token }}\n",
    "{% endfor -%}\n",
    "{% if add_generation_prompt and messages[-1]['role'] != 'assistant' -%}\n",
    "{{ bos_token }}assistant\n",
    "{% endif -%}\"\"\"\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "print(\"Loading model\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Tokenizer BOS: {tokenizer.bos_token} ({tokenizer.bos_token_id}), EOS: {tokenizer.eos_token} ({tokenizer.eos_token_id}), PAD: {tokenizer.pad_token} ({tokenizer.pad_token_id})\")\n",
    "print(f\"Model BOS: {model.config.bos_token_id}, EOS: {model.config.eos_token_id}, PAD: {model.config.pad_token_id}\")\n",
    "assert model.config.pad_token_id is not None, \"Model's pad_token_id is not set!\"\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading ALPACA…\")\n",
    "alpaca_split = load_dataset(ALPACA_DATASET_NAME, split=\"train\").train_test_split(test_size=0.1, seed=SEED, shuffle=True)\n",
    "alpaca_train = alpaca_split[\"train\"]\n",
    "alpaca_val = alpaca_split[\"test\"]\n",
    "\n",
    "def format_alpaca(ds_split):\n",
    "    convos = []\n",
    "    for ex in ds_split:\n",
    "        instr = ex.get(\"instruction\", \"\").strip()\n",
    "        ctx = ex.get(\"input\", \"\").strip()\n",
    "        response = ex.get(\"output\", \"\").strip()\n",
    "\n",
    "        user_msg_parts = [instr]\n",
    "        if ctx: # Add context only if it's not empty\n",
    "            user_msg_parts.append(ctx)\n",
    "        user_msg = \"\\n\\n\".join(user_msg_parts)\n",
    "\n",
    "        if user_msg and response:\n",
    "            convos.append([\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "                {\"role\": \"assistant\", \"content\": response},\n",
    "            ])\n",
    "    return convos\n",
    "\n",
    "\n",
    "train_alpaca_convos = format_alpaca(alpaca_train)\n",
    "val_alpaca_convos = format_alpaca(alpaca_val)\n",
    "print(f\"Alpaca -> {len(train_alpaca_convos)} train / {len(val_alpaca_convos)} val convos\")\n",
    "\n",
    "print(\"Loading Dolly 15k…\")\n",
    "dolly = load_dataset(DOLLY_DATASET_NAME)\n",
    "\n",
    "# Filter out examples with missing 'instruction' or 'response' in Dolly\n",
    "dolly_filtered = dolly.filter(lambda ex: ex.get(\"instruction\") and ex.get(\"response\") and \\\n",
    "                                       len(ex[\"instruction\"].strip()) > 0 and \\\n",
    "                                       len(ex[\"response\"].strip()) > 0)\n",
    "\n",
    "dolly_split = dolly_filtered[\"train\"].train_test_split(test_size=0.1, seed=SEED, shuffle=True)\n",
    "dolly_train = dolly_split[\"train\"]\n",
    "dolly_val = dolly_split[\"test\"]\n",
    "\n",
    "def format_dolly(ds_split):\n",
    "    convos = []\n",
    "    for ex in ds_split:\n",
    "        instr = ex.get(\"instruction\", \"\").strip()\n",
    "        ctx = ex.get(\"context\", \"\").strip()\n",
    "        response = ex.get(\"response\", \"\").strip()\n",
    "\n",
    "        user_msg_parts = [instr]\n",
    "        if ctx:\n",
    "            user_msg_parts.append(ctx)\n",
    "        user_msg = \"\\n\\n\".join(user_msg_parts)\n",
    "\n",
    "        if user_msg and response:\n",
    "            convos.append([\n",
    "                {\"role\": \"user\", \"content\": user_msg},\n",
    "                {\"role\": \"assistant\", \"content\": response},\n",
    "            ])\n",
    "    return convos\n",
    "\n",
    "train_dolly_convos = format_dolly(dolly_train)\n",
    "val_dolly_convos = format_dolly(dolly_val)\n",
    "print(f\"Dolly-15k -> {len(train_dolly_convos)} train / {len(val_dolly_convos)} val convos\")\n",
    "\n",
    "# combine the 2 datasets\n",
    "all_train_convos = train_alpaca_convos + train_dolly_convos\n",
    "all_val_convos = val_alpaca_convos + val_dolly_convos\n",
    "print(f\"Combined (raw) -> {len(all_train_convos)} train / {len(all_val_convos)} val convos\")\n",
    "\n",
    "# remove those > context length\n",
    "def filter_and_prepare_conversations(convos, tokenizer_ref, max_len):\n",
    "    filtered_convos = []\n",
    "    for convo in tqdm(convos, desc=\"Filtering long conversations\"):\n",
    "        if not convo: continue # skip empty conversation\n",
    "        prompt_text = tokenizer_ref.apply_chat_template(\n",
    "            convo,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        tokenized_len = len(tokenizer_ref.encode(prompt_text, truncation=False))\n",
    "\n",
    "        if tokenized_len > 0 and tokenized_len <= max_len:\n",
    "            filtered_convos.append({\"conversations\": convo})\n",
    "        elif tokenized_len == 0:\n",
    "            print(f\"Empty conversation {convo}\")\n",
    "\n",
    "    return filtered_convos\n",
    "\n",
    "train_convos_filtered = filter_and_prepare_conversations(all_train_convos, tokenizer, MAX_LENGTH)\n",
    "val_convos_filtered = filter_and_prepare_conversations(all_val_convos, tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"Filtered by length -> {len(train_convos_filtered)} train / {len(val_convos_filtered)} val convos\")\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_list(train_convos_filtered)\n",
    "val_dataset = Dataset.from_list(val_convos_filtered)\n",
    "\n",
    "\n",
    "ASSISTANT_ROLE_NAME = \"assistant\"\n",
    "USER_ROLE_NAME = \"user\"\n",
    "\n",
    "def formatting_func(example):\n",
    "    # 'example' is something like {\"conversations\": [{\"role\": ..., \"content\": ...}, ...]}\n",
    "    conversation = example[\"conversations\"]\n",
    "\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        truncation=True, # not really needed here bcs we already remove those that > max length\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    input_ids = tokenized_inputs[\"input_ids\"]\n",
    "\n",
    "    labels = [-100] * len(input_ids)\n",
    "    \n",
    "    # find the last assistant response\n",
    "    last_assistant_idx = max(\n",
    "        idx for idx, turn in enumerate(conversation)\n",
    "        if turn[\"role\"] == ASSISTANT_ROLE_NAME\n",
    "    )\n",
    "\n",
    "    # Iterate through the tokenized input_ids to unmask assistant content\n",
    "    \n",
    "    current_token_idx = 0\n",
    "    for turn_idx, turn in enumerate(conversation):\n",
    "        role = turn[\"role\"]\n",
    "        content = turn[\"content\"]\n",
    "\n",
    "        try:\n",
    "            start_of_turn_bos_idx = input_ids.index(tokenizer.bos_token_id, current_token_idx)\n",
    "        except ValueError:\n",
    "            break \n",
    "\n",
    "        search_for_eos_from = start_of_turn_bos_idx + 1 # Search after the current BOS\n",
    "        end_of_turn_eos_idx = -1\n",
    "\n",
    "        for k_eos in range(search_for_eos_from, len(input_ids)):\n",
    "            if input_ids[k_eos] == tokenizer.eos_token_id:\n",
    "                end_of_turn_eos_idx = k_eos\n",
    "                break\n",
    "        if end_of_turn_eos_idx == -1:\n",
    "            print(f\"Warning: Could not find EOS token for turn: {turn}\")\n",
    "            return None\n",
    "\n",
    "        role_and_newline_text = f\"{role}\\n\"\n",
    "        role_and_newline_tokens = tokenizer.encode(role_and_newline_text, add_special_tokens=False)\n",
    "\n",
    "        start_of_content_idx = start_of_turn_bos_idx + 1 + len(role_and_newline_tokens) # +1 for BOS\n",
    "\n",
    "        if role == ASSISTANT_ROLE_NAME and turn_idx == last_assistant_idx:\n",
    "            # Unmask tokens from start_of_content_idx up to (but not including) end_of_turn_eos_idx\n",
    "                for k_label in range(start_of_content_idx, end_of_turn_eos_idx):\n",
    "                    if k_label >= 0 and k_label < len(labels):\n",
    "                        labels[k_label] = input_ids[k_label]\n",
    "        \n",
    "        current_token_idx = end_of_turn_eos_idx + 1 # Move search for next turn after this turn's EOS\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": tokenized_inputs[\"attention_mask\"],\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "tokenized_train = train_dataset.map(formatting_func)\n",
    "tokenized_val = val_dataset.map(formatting_func)\n",
    "\n",
    "# Set format\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train['conversations'][192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import types\n",
    "\n",
    "def add_neftune(embedding_layer, noise_alpha):\n",
    "    original_forward = embedding_layer.forward\n",
    "\n",
    "    def new_forward(self, input_ids):\n",
    "        if self.training:\n",
    "            embed_init = original_forward(input_ids)\n",
    "            L = input_ids.size(1)\n",
    "            d = embed_init.size(2)\n",
    "            mag_norm = noise_alpha / (L * d)**0.5\n",
    "            # generate uniform noise\n",
    "            noise = torch.zeros_like(embed_init).uniform_(-mag_norm, mag_norm)\n",
    "            return embed_init + noise\n",
    "        else:\n",
    "            # during inference, return standard embeddings without noise\n",
    "            return original_forward(input_ids)\n",
    "\n",
    "    # bind the new forward method to the embedding layer\n",
    "    embedding_layer.forward = types.MethodType(new_forward, embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984d9869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = model.get_input_embeddings()\n",
    "add_neftune(embedding_layer, noise_alpha=NEFTUNE_NOISE_ALPHA)\n",
    "\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=500,\n",
    "    save_total_limit=6,\n",
    "    logging_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    "    remove_unused_columns=False,\n",
    "    torch_compile=True,\n",
    "    seed=SEED,\n",
    "    report_to=\"wandb\",\n",
    "    run_name = \"gpt2-chat-0p3\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning…\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf06595",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_MODEL_PATH = './gpt2-instruct'\n",
    "trainer.save_model(FINAL_MODEL_PATH)\n",
    "tokenizer.save_pretrained(FINAL_MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
