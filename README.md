# llm-from-scratch
Reproduce GPT-2 and instruct-following finetuning.

## Description
This is the source code for the [Reproducing GPT-2 from scratch blog post](https://lchenghui.com/llm-from-scratch).

The notebooks are ordered in their logical sequence:
1. Train the tokenizer, tokenize the dataset, and train the model.
2. Evaluate the model on the validation set.
3. Finetune the model on the instruction-following dataset.
