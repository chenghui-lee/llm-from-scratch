{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "739d8b31",
   "metadata": {},
   "source": [
    "# Calculate loss for base model checkpoints\n",
    "\n",
    "Since I didn't include a validation set during pre-training, this notebook is to find the best performing checkpoints using (hopefully) unseen data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3d27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# login to huggingface to avoid rate-limit\n",
    "!huggingface-cli login --token <token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385cfa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"/root/fineweb-gpt2-356m/checkpoint-9652\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "streaming_eval = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    \"sample-100BT\",\n",
    "    split=\"train\",\n",
    "    streaming=True\n",
    ").shuffle(buffer_size=10_000, seed=3047).take(3000)\n",
    "eval_list    = list(streaming_eval)\n",
    "raw_dataset  = Dataset.from_list(eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda1957",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 1024\n",
    "\n",
    "def concatenate_and_chunk(element):\n",
    "    all_token_ids = []\n",
    "    for text in element[\"text\"]:\n",
    "        token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "        all_token_ids.extend(token_ids)\n",
    "        all_token_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    total_length = len(all_token_ids)\n",
    "\n",
    "    if total_length < context_length:\n",
    "        return {\"input_ids\": [], \"labels\": []}\n",
    "\n",
    "    total_length = (total_length // context_length) * context_length\n",
    "\n",
    "    # Split the concatenated tokens into chunks of context_length\n",
    "    chunks_input_ids = []\n",
    "    for i in range(0, total_length, context_length):\n",
    "        chunk = all_token_ids[i : i + context_length]\n",
    "        if len(chunk) == context_length:\n",
    "            chunks_input_ids.append(chunk)\n",
    "\n",
    "    output = {\"input_ids\": chunks_input_ids, \"labels\": chunks_input_ids.copy()}\n",
    "    return output\n",
    "\n",
    "\n",
    "raw_dataset = raw_dataset.remove_columns(\n",
    "    [col for col in raw_dataset.column_names if col != \"text\"]\n",
    ")\n",
    "\n",
    "tokenized = raw_dataset.map(\n",
    "    concatenate_and_chunk,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset.column_names,\n",
    "    num_proc=os.cpu_count(),\n",
    ")\n",
    "\n",
    "# remove those invalid rows\n",
    "tokenized = tokenized.filter(lambda ex: len(ex[\"input_ids\"]) > 0)\n",
    "\n",
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f58b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "eval_dataloader = DataLoader(tokenized, batch_size=32, shuffle=False)\n",
    "\n",
    "checkpoint_dir = \"./fineweb-gpt2-356m\"\n",
    "checkpoint_folders = [\n",
    "    f.path for f in os.scandir(checkpoint_dir)\n",
    "    if f.is_dir() and f.name.startswith(\"checkpoint-\")\n",
    "]\n",
    "checkpoint_folders.sort(key=lambda x: int(x.split(\"-\")[-1]))\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "results = {}\n",
    "\n",
    "for checkpoint_path in checkpoint_folders:\n",
    "    print(f\"Evaluating {checkpoint_path}\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(checkpoint_path).to(device)\n",
    "        model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(eval_dataloader, desc=\"batches\", leave=False):\n",
    "                batch = {k: v.to(device) for k, v in batch.items()}\n",
    "                outputs = model(**batch)\n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches if num_batches else float(\"nan\")\n",
    "        results[checkpoint_path] = avg_loss\n",
    "        print(f\"  â†’ avg_loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # cleanup the memory\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error at {checkpoint_path}: {e}\")\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "for ckpt, loss in results.items():\n",
    "    print(f\"{os.path.basename(ckpt)}: {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
